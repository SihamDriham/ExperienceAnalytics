services:
  # Apache Zookeeper
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_HEAP_SIZE=256
    volumes:
      - zookeeper_data:/bitnami/zookeeper
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - data-pipeline

  # Apache Kafka
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    hostname: kafka  # Hostname fixe
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT_INTERNAL
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_NUM_PARTITIONS=3
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_CFG_LOG_RETENTION_HOURS=168
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS=300000
      - KAFKA_HEAP_OPTS=-Xmx512m -Xms512m
      - ALLOW_PLAINTEXT_LISTENER=yes
      
      # Configuration Zookeeper avec timeouts ajustés
      - KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS=30000  # 30 secondes
      - KAFKA_CFG_ZOOKEEPER_SESSION_TIMEOUT_MS=30000     # 30 secondes
      - KAFKA_CFG_ZOOKEEPER_SYNC_TIME_MS=10000           # 10 secondes
      
      # Configuration de reconnexion
      - KAFKA_CFG_ZOOKEEPER_MAX_IN_FLIGHT_REQUESTS=10
      - KAFKA_CFG_CONNECTIONS_MAX_IDLE_MS=540000         # 9 minutes
      
    volumes:
      - kafka_data:/bitnami/kafka
      
    depends_on:
      zookeeper:
        condition: service_healthy
        
    # Healthcheck avec plus de tolérance
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 45s
      timeout: 15s
      retries: 5
      start_period: 90s  # Plus de temps pour que Zookeeper soit stable
      
    restart: unless-stopped
    networks:
      - data-pipeline
      
  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    ports:
      - "9042:9042"
      - "7000:7000"
    environment:
      - CASSANDRA_CLUSTER_NAME=DataPipeline
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_SEEDS=cassandra
      - CASSANDRA_LISTEN_ADDRESS=auto
      - CASSANDRA_BROADCAST_ADDRESS=auto
      - CASSANDRA_RPC_ADDRESS=0.0.0.0
      - CASSANDRA_START_RPC=true
      - MAX_HEAP_SIZE=1G
      - HEAP_NEWSIZE=200M
    volumes:
      - cassandra_data:/var/lib/cassandra
    healthcheck:
      test: ["CMD-SHELL", "nodetool status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    networks:
      - data-pipeline
    ulimits:
      memlock: -1
      nofile:
        soft: 100000
        hard: 100000

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    depends_on:
      cassandra:
        condition: service_healthy
    # IMPORTANT: Remettre l'utilisateur pour éviter les problèmes d'authentification
    user: "1001:1001"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_OPTS=-Dspark.deploy.defaultCores=2
      # Configuration Hadoop complète pour Windows
      - HADOOP_USER_NAME=spark
      - USER=spark
      - SPARK_USER=spark
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_LOCAL_HOSTNAME=spark-master
      # Variables d'environnement système pour Windows/Docker
      - LOGNAME=spark
      - USERNAME=spark
      # Configuration pour éviter les problèmes de ligne de commande Windows
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8083:8080"
      - "7077:7077"
    volumes:
      # Attention: sous Windows, utilisez des chemins relatifs ou absolus Windows
      - ./scripts:/opt/bitnami/spark/scripts
      - ./data:/opt/bitnami/spark/data
      - ./logs:/opt/bitnami/spark/logs
      - spark_logs:/opt/bitnami/spark/logs
      - ./config/spark:/opt/bitnami/spark/conf
      # Fichier passwd personnalisé pour résoudre l'authentification Unix
      - ./config/passwd:/etc/passwd:ro
    command: >
      bash -c "
        echo 'Attente de Cassandra...'
        until timeout 1 bash -c '</dev/tcp/cassandra/9042' 2>/dev/null; do
          echo 'Cassandra pas encore prêt, attente...'
          sleep 2
        done
        echo 'Cassandra est prêt!'
        # Démarrer Spark Master
        /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - data-pipeline

  # Apache Spark Worker
  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_healthy
      cassandra:
        condition: service_healthy
    # IMPORTANT: Remettre l'utilisateur
    user: "1001:1001"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=-Dspark.worker.cleanup.enabled=true
      # Configuration Hadoop complète
      - HADOOP_USER_NAME=spark
      - USER=spark
      - SPARK_USER=spark
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_LOCAL_HOSTNAME=spark-worker
      # Variables d'environnement système
      - LOGNAME=spark
      - USERNAME=spark
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
      - ./data:/opt/bitnami/spark/data
      - ./logs:/opt/bitnami/spark/logs
      - spark_logs:/opt/bitnami/spark/logs
      - ./config/spark:/opt/bitnami/spark/conf
      - ./config/passwd:/etc/passwd:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - data-pipeline

  # Redis pour Airflow
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    networks:
      - data-pipeline

  # PostgreSQL pour Airflow
  postgres-airflow:
    image: postgres:15
    container_name: postgres-airflow
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - data-pipeline

  # Airflow Init (pour l'initialisation)
  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    depends_on:
      postgres-airflow:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment: &airflow-common-env
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres-airflow:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=81HqDtbqAywKSOumSHA3BhWNOdQ26slT6K0YaZeZyPs=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
      - _AIRFLOW_WWW_USER_EMAIL=admin@example.com
    volumes: &airflow-common-volumes
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
    user: "50000:0"
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
      "
    networks:
      - data-pipeline

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.8.0
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    user: "50000:0"
    networks:
      - data-pipeline

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.8.0
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    user: "50000:0"
    networks:
      - data-pipeline

  # Airflow Worker
  airflow-worker:
    image: apache/airflow:2.8.0
    container_name: airflow-worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    command: celery worker
    healthcheck:
      test: ["CMD-SHELL", "celery --app airflow.executors.celery_executor.app inspect ping -d celery@$${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    user: "50000:0"
    networks:
      - data-pipeline

  # Airflow Flower (monitoring)
  airflow-flower:
    image: apache/airflow:2.8.0
    container_name: airflow-flower
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment: *airflow-common-env
    volumes: *airflow-common-volumes
    ports:
      - "5555:5555"
    command: celery flower
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    user: "50000:0"
    networks:
      - data-pipeline

  # Grafana pour la visualisation
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,hadesarchitect-cassandra-datasource

    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana:/etc/grafana/provisioning
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - data-pipeline

networks:
  data-pipeline:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  zookeeper_data:
  kafka_data:
  postgres_data:
  postgres_airflow_data:
  redis_data:
  spark_logs:
  cassandra_data:
  grafana_data: